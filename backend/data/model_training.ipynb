{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7983b73-d489-4769-b048-18d2e095e0ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7ffec97d-86a7-4761-b94b-7315f8bb4998",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Bangalore dataset loaded successfully.\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3000 entries, 0 to 2999\n",
      "Data columns (total 13 columns):\n",
      " #   Column                     Non-Null Count  Dtype  \n",
      "---  ------                     --------------  -----  \n",
      " 0   Latitude                   3000 non-null   float64\n",
      " 1   Longitude                  3000 non-null   float64\n",
      " 2   Altitude                   3000 non-null   float64\n",
      " 3   Rainfall_Intensity         3000 non-null   float64\n",
      " 4   Temperature                3000 non-null   float64\n",
      " 5   Humidity                   3000 non-null   float64\n",
      " 6   Atmospheric_Pressure       3000 non-null   float64\n",
      " 7   River_Level                3000 non-null   float64\n",
      " 8   Drainage_Capacity          3000 non-null   float64\n",
      " 9   Drainage_System_Condition  3000 non-null   int64  \n",
      " 10  Population_Density         3000 non-null   float64\n",
      " 11  Urbanization_Level         3000 non-null   int64  \n",
      " 12  flood                      3000 non-null   int64  \n",
      "dtypes: float64(10), int64(3)\n",
      "memory usage: 304.8 KB\n"
     ]
    }
   ],
   "source": [
    "# 1. Define the correct path to the dataset\n",
    "file_path = 'bangalore_urban_flood_prediction.csv'\n",
    "\n",
    "# 2. Load the data\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "print(\"✅ Bangalore dataset loaded successfully.\")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e4566a4d-9d58-4b6d-8bd0-4d9d14b01700",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target variable: flood\n",
      "\n",
      "Features used for training: ['Latitude', 'Longitude', 'Altitude', 'Rainfall_Intensity', 'Temperature', 'Humidity', 'Atmospheric_Pressure', 'River_Level', 'Drainage_Capacity', 'Drainage_System_Condition', 'Population_Density', 'Urbanization_Level']\n"
     ]
    }
   ],
   "source": [
    "# The 'flood' column is what we want to predict (0 for no, 1 for yes)\n",
    "target_name = 'flood'\n",
    "\n",
    "# All other columns will be used as features\n",
    "feature_names = df.columns.drop(target_name).tolist()\n",
    "\n",
    "X = df[feature_names]\n",
    "y = df[target_name]\n",
    "\n",
    "print(\"Target variable:\", target_name)\n",
    "print(\"\\nFeatures used for training:\", feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f52934-82e7-4b22-bba7-c8b69de41668",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.11.7' requires the ipykernel package.\n",
      "\u001b[1;31m<a href='command:jupyter.createPythonEnvAndSelectController'>Create a Python Environment</a> with the required packages.\n",
      "\u001b[1;31mOr install 'ipykernel' using the command: 'c:/msys64/ucrt64/bin/python.exe -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# 1. Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# 2. Initialize and fit the scaler on the training data ONLY\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test) # Use the same scaler for the test data\n",
    "\n",
    "print(\"✅ Data has been split and scaled.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b022a2e2-b816-4274-b824-260e0ae822bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Bangalore-specific model training complete with balanced class weights.\n"
     ]
    }
   ],
   "source": [
    "# 1. Initialize the model WITH the class_weight parameter\n",
    "# This is the crucial change that will fix the bias.\n",
    "model = RandomForestClassifier(\n",
    "    n_estimators=100, \n",
    "    random_state=42, \n",
    "    n_jobs=-1,\n",
    "    class_weight='balanced'  # <--- THE FIX IS HERE\n",
    ")\n",
    "\n",
    "# 2. Train the model\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"✅ Bangalore-specific model training complete with balanced class weights.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4aec2524-d912-43c8-9364-cb8de50a9d73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model saved to 'ml_artifacts/model.pkl'\n",
      "✅ Scaler saved to 'ml_artifacts/scaler.pkl'\n",
      "✅ Feature list saved to 'ml_artifacts/model_features.pkl'\n"
     ]
    }
   ],
   "source": [
    "# 1. Define the directory to save the model files\n",
    "artifacts_dir = 'ml_artifacts'\n",
    "os.makedirs(artifacts_dir, exist_ok=True)\n",
    "\n",
    "# 2. Save the trained model\n",
    "with open(os.path.join(artifacts_dir, 'model.pkl'), 'wb') as f:\n",
    "    pickle.dump(model, f)\n",
    "print(f\"✅ Model saved to '{os.path.join(artifacts_dir, 'model.pkl')}'\")\n",
    "\n",
    "# 3. Save the fitted scaler\n",
    "with open(os.path.join(artifacts_dir, 'scaler.pkl'), 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "print(f\"✅ Scaler saved to '{os.path.join(artifacts_dir, 'scaler.pkl')}'\")\n",
    "\n",
    "# 4. Save the list of feature names\n",
    "with open(os.path.join(artifacts_dir, 'model_features.pkl'), 'wb') as f:\n",
    "    pickle.dump(feature_names, f)\n",
    "print(f\"✅ Feature list saved to '{os.path.join(artifacts_dir, 'model_features.pkl')}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2755a76-a389-4c19-b01e-70dea8e4c861",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
